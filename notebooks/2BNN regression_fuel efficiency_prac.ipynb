{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "#   plt.ylim([0, 10])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [MPG]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset.copy()\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Origin\" column is really categorical, not numeric. So convert that to a one-hot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 1: Train/test split\n",
    "We'll separate validation data from the training set when calling `model.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the training/test data size\n",
    "print(\"the training data size\", train_dataset.shape)\n",
    "print(\"the test data size\", test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Split `features` from `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('MPG')\n",
    "test_labels = test_features.pop('MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "- We have 9 features and 1 response variable;\n",
    "- We will do a regression problem;\n",
    "- We have small dataset, which only contains 314 cases;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "featurewise normalization written by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traditional feature-wise normalization\n",
    "print(train_features.mean(axis=0))\n",
    "print(train_features.std(axis=0))\n",
    "\n",
    "train_mean = train_features.mean(axis=0)\n",
    "train_std = train_features.std(axis=0)\n",
    "\n",
    "train_features = (train_features - train_mean) / train_std\n",
    "test_features = (test_features - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pd.DataFrame to np.array\n",
    "\n",
    "x_train  = np.array(train_features)\n",
    "x_test  = np.array(test_features)\n",
    "y_train  = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the shapes of data\n",
    "print(\"training features\", x_train.shape)\n",
    "print(\"training labels\", y_train.shape)\n",
    "print(\"test features\", x_test.shape)\n",
    "print(\"test labels\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 1. Mitiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_model = tf.keras.Sequential([\n",
    "#     layers.Dense(1, input_shape=(9,))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_model.compile(\n",
    "#     optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "#     loss='mean_absolute_error')\n",
    "# #%%time\n",
    "# history = linear_model.fit(\n",
    "#     train_features, train_labels, \n",
    "#     epochs=100,\n",
    "#     # suppress logging\n",
    "#     verbose=0,\n",
    "#     # Calculate validation results on 20% of the training data\n",
    "#     validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show the 'MAE' on test data\n",
    "# test_results['linear_model'] = linear_model.evaluate(test_features, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Plot for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tst_pred = linear_model.predict(test_features)\n",
    "# tst_pred.flatten();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,8))\n",
    "# a = plt.axes(aspect='equal')\n",
    "# plt.scatter(test_labels, tst_pred)\n",
    "# plt.xlabel('Ground Truth [MPG]')\n",
    "# plt.ylabel('Predictions [MPG]')\n",
    "# lims = [0, 50]\n",
    "# plt.xlim(lims)\n",
    "# plt.ylim(lims)\n",
    "# _ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### $R^2$ for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_model_r2 = r2_score(test_labels, tst_pred)\n",
    "# linear_model_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 2. Deterministic DNN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture of the DNN:\n",
    "- Two hidden, nonlinear, `Dense` layer using the `relu` function\n",
    "- A linear single output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_model = keras.Sequential([\n",
    "      layers.Dense(64, activation='relu', input_shape=(9,)),\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(1)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_model.compile(loss='mse',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001) , metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = DNN_model.fit(train_features, train_labels, \n",
    "    epochs=300,\n",
    "    # suppress logging\n",
    "    verbose=2,\n",
    "    # Calculate validation results on 20% of the training data\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Performance check\n",
    "\n",
    "1. 'mae' value based on test data set\n",
    "2. $R^{2}$ on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results['DNN_model'] = DNN_model.evaluate(x_test,y_test, verbose=0)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_pred = DNN_model.predict(x_test)\n",
    "tst_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_pred = DNN_model.predict(x_test)\n",
    "# tst_pred.flatten()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "a = plt.axes(aspect='equal')\n",
    "\n",
    "plt.scatter(test_labels, tst_pred)\n",
    "plt.xlabel('Ground Truth [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### $R^2$ for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_model_r2 = r2_score(test_labels, tst_pred)\n",
    "DNN_model_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## 3. Aleatoric DNN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "      layers.Dense(64, activation='relu', input_shape=(9,)),\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      Dense(2),\n",
    "      tfpl.IndependentNormal(1)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the \"negative log likelihood\" as loss function\n",
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=nll,\n",
    "             optimizer='adam',\n",
    "             metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_features, train_labels, validation_split=0.2, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictive_mean = y_preds.mean().numpy()\n",
    "y_std = np.squeeze(y_preds.stddev().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_mean;\n",
    "# y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot([5,45], [5,45], 'k--')\n",
    "plt.errorbar(test_labels, y_predictive_mean, yerr=y_std, fmt='o', color='blue', ecolor='lightblue', elinewidth=3, capsize=0)\n",
    "plt.xlabel('Ground truth')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Confidence interval of predictions on test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$ for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_model_r2 = r2_score(test_labels, y_predictive_mean)\n",
    "DNN_model_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mae metric for test data\n",
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## 4. Bayesian DNN model via `DenseVariational`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. define prior\n",
    "2. define posterior (w/ or w/o covariances)\n",
    "3. create the Sequential model structure with `DenseVariational` layer\n",
    "4. define the loss function\n",
    "5. train: model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "print('TF version:', tf.__version__)\n",
    "print('TFP version:', tfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indenpendent prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prior weight distribution -- all N(0, 1) -- and not trainable\n",
    "\n",
    "def prior(kernel_size, bias_size, dtype=None):\n",
    "    # number of parameters\n",
    "    n = kernel_size + bias_size\n",
    "    prior_model = Sequential([\n",
    "        tfpl.DistributionLambda(\n",
    "        lambda t : tfd.Independent(tfd.Normal(loc=tf.zeros(n, dtype=dtype), scale=1),\n",
    "                                  reinterpreted_batch_ndims=1))\n",
    "    ])\n",
    "    return prior_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variational posterior weight distribution -- multivariate Gaussian\n",
    "\n",
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size \n",
    "    posterior_model = Sequential([\n",
    "        tfpl.VariableLayer(tfpl.IndependentNormal.params_size(n), dtype=dtype),\n",
    "        tfpl.IndependentNormal(n, convert_to_tensor_fn=tfd.Distribution.sample)\n",
    "    ])\n",
    "    return posterior_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Create the model with `DenseVariational` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probabilistic regression with one hidden layer, weight uncertainty\n",
    "\n",
    "model = Sequential([\n",
    "    tfpl.DenseVariational(units=64,\n",
    "                          input_shape=(9,),\n",
    "                          make_prior_fn=prior,\n",
    "                          make_posterior_fn=posterior,\n",
    "                          kl_weight=1/train_features.shape[0],\n",
    "                          activation='relu'),\n",
    "    tfpl.DenseVariational(units=64,\n",
    "                          make_prior_fn=prior,\n",
    "                          make_posterior_fn=posterior,\n",
    "                          kl_weight=1/train_features.shape[0],\n",
    "                          activation='relu'),\n",
    "    tfpl.DenseVariational(units=1,\n",
    "                          make_prior_fn=prior,\n",
    "                          make_posterior_fn=posterior,\n",
    "                          kl_weight=1/train_features.shape[0]),\n",
    "    tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment:*\n",
    "- Look at the shape of the output distribution, it says batch_shape as [78, 1]. \n",
    "- we can further add tfd.Independent to the last layer to make it [78,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "BNN_history = model.fit(train_features, train_labels, epochs=500, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(BNN_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results['BNN_model'] = model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the predictions of BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick look at the test data\n",
    "test_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a random draw from weights' distributions and do a one time naive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_tst = model.predict(test_features)\n",
    "y_tst = model(test_features[0][np.newaxis])\n",
    "# print('type', y_tst)\n",
    "# print('shape', y_tst.shape)\n",
    "print(y_tst.mean().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inspect the shape of predictions\n",
    "# y_tst = model(test_features)\n",
    "# y_tst.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hand calculated the MAE error of the test data\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# mean_absolute_error(test_labels.to_numpy(), y_tst.mean().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictive distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply record the results and w/o plotting (hight dimensional cases)\n",
    "import tqdm\n",
    "\n",
    "y_pred_list = []\n",
    "\n",
    "for _ in tqdm.tqdm(range(1000)):\n",
    "    y_pred = model(test_features)\n",
    "    y_pred_list.append(y_pred.mean().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.concatenate(y_pred_list, axis=1)\n",
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = np.mean(y_preds, axis=1)\n",
    "y_sigma = np.std(y_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot([5,45], [5,45], 'k--')\n",
    "plt.errorbar(test_labels.to_numpy(), y_mean, yerr=y_sigma, fmt='o', color='blue', ecolor='lightblue', elinewidth=3, capsize=0)\n",
    "plt.xlabel('Ground truth')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Results of test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. MC Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working on a scalar regression task. Instead of a single point estimate for the final prediction $y^{*}$, given a vecor of features $\\mathbf{x}$, we now have a predictive distribution on $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a Dropout model first\n",
    "\n",
    "model_mc = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(9,)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc.compile(loss='mse',\n",
    "             optimizer='adam',\n",
    "             metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mc = model_mc.fit(x_train, y_train, validation_split=0.2, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file demonstrates how to toggle MC Dropout in keras\n",
    "# this answer is sourced from https://stackoverflow.com/questions/63238203/how-to-get-intermediate-outputs-in-tf-2-3-eager-with-learning-phase\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.backend import eager_learning_phase_scope\n",
    "from tensorflow.python.keras import backend as K\n",
    "f = K.function([model_mc.input], [model_mc.output])\n",
    "\n",
    "# run in training mode, i.e. 1 means training\n",
    "# this code is for a single input\n",
    "with eager_learning_phase_scope(value=1):\n",
    "    output_train = f(x_test[0][np.newaxis])[0]\n",
    "    \n",
    "print(output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conceptually, show the predictive distribution for a single input vector $\\mathbf{x}^{*}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run in test mode, i.e. 0 means test\n",
    "# with eager_learning_phase_scope(value=0):\n",
    "#     output_test = f([x])\n",
    "\n",
    "# Run the function for the number of mc_samples with learning_phase enabled\n",
    "# What's important is the the learning phase. When set to 0, all weights are fixed; when set to 1, dropout is used at test time\n",
    "\n",
    "with eager_learning_phase_scope(value=1): # 0=test, 1=train\n",
    "    Y_hat_mc = np.squeeze(np.array([(f(x_test)[0])[0] for _ in range(50)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for the whole test data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with eager_learning_phase_scope(value=1): # 0=test, 1=train\n",
    "    Y_hat_mc = np.squeeze(np.array([(f(x_test))[0] for _ in range(50)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_mc[0]  == Y_hat_mc[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Bayesian DNN model via `Flipout`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probabilistic regression with one hidden layer, weight uncertainty\n",
    "\n",
    "kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (train_features.shape[0] * 1.0)\n",
    "bias_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (x.shape.shape[0] * 1.0)\n",
    "\n",
    "\n",
    "model_flipout = Sequential([\n",
    "    tfpl.DenseFlipout(units=64, input_shape=(9,),activation='relu',\n",
    "                      kernel_divergence_fn=kernel_divergence_fn,\n",
    "                      bias_divergence_fn=bias_divergence_fn,),\n",
    "    tfpl.DenseFlipout(1,\n",
    "                      kernel_divergence_fn=kernel_divergence_fn,\n",
    "                      bias_divergence_fn=bias_divergence_fn,),\n",
    "    tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "model_flipout.compile(loss=nll, optimizer=RMSprop(learning_rate=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flipout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flipout.fit(train_features, train_labels, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
